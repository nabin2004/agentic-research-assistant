id,title,authors,published,pdf_url,summary,doi,venue,publisher
http://arxiv.org/abs/2101.06480v1,SelfMatch: Combining Contrastive Self-Supervision and Consistency for Semi-Supervised Learning,Byoungjip Kim; Jinho Choo; Yeong-Dae Kwon; Seongho Joe; Seungjai Min; Youngjune Gwon,2021-01-16,https://arxiv.org/pdf/2101.06480v1,"This paper introduces SelfMatch, a semi-supervised learning method that combines the power of contrastive self-supervised learning and consistency regularization. SelfMatch consists of two stages: (1) self-supervised pre-training based on contrastive learning and (2) semi-supervised fine-tuning based on augmentation consistency regularization. We empirically demonstrate that SelfMatch achieves the state-of-the-art results on standard benchmark datasets such as CIFAR-10 and SVHN. For example, for CIFAR-10 with 40 labeled examples, SelfMatch achieves 93.19% accuracy that outperforms the strong previous methods such as MixMatch (52.46%), UDA (70.95%), ReMixMatch (80.9%), and FixMatch (86.19%). We note that SelfMatch can close the gap between supervised learning (95.87%) and semi-supervised learning (93.19%) by using only a few labels for each class.",10.1016/j.patrec.2022.08.006,Pattern Recognition Letters,Elsevier BV
http://arxiv.org/abs/2008.10312v2,Self-Supervised Learning for Large-Scale Unsupervised Image Clustering,Evgenii Zheltonozhskii; Chaim Baskin; Alex M. Bronstein; Avi Mendelson,2020-08-24,https://arxiv.org/pdf/2008.10312v2,"Unsupervised learning has always been appealing to machine learning researchers and practitioners, allowing them to avoid an expensive and complicated process of labeling the data. However, unsupervised learning of complex data is challenging, and even the best approaches show much weaker performance than their supervised counterparts. Self-supervised deep learning has become a strong instrument for representation learning in computer vision. However, those methods have not been evaluated in a fully unsupervised setting. In this paper, we propose a simple scheme for unsupervised classification based on self-supervised representations. We evaluate the proposed approach with several recent self-supervised methods showing that it achieves competitive results for ImageNet classification (39% accuracy on ImageNet with 1000 clusters and 46% with overclustering). We suggest adding the unsupervised evaluation to a set of standard benchmarks for self-supervised learning. The code is available a",10.1109/tcsvt.2012.2202079,IEEE Transactions on Circuits and Systems for Video Technology,Institute of Electrical and Electronics Engineers (IEEE)
http://arxiv.org/abs/2210.16365v1,Elastic Weight Consolidation Improves the Robustness of Self-Supervised Learning Methods under Transfer,Andrius Ovsianas; Jason Ramapuram; Dan Busbridge; Eeshan Gunesh Dhekane; Russ Webb,2022-10-28,https://arxiv.org/pdf/2210.16365v1,"Self-supervised representation learning (SSL) methods provide an effective label-free initial condition for fine-tuning downstream tasks. However, in numerous realistic scenarios, the downstream task might be biased with respect to the target label distribution. This in turn moves the learned fine-tuned model posterior away from the initial (label) bias-free self-supervised model posterior. In this work, we re-interpret SSL fine-tuning under the lens of Bayesian continual learning and consider regularization through the Elastic Weight Consolidation (EWC) framework. We demonstrate that self-regularization against an initial SSL backbone improves worst sub-group performance in Waterbirds by 5% and Celeb-A by 2% when using the ViT-B/16 architecture. Furthermore, to help simplify the use of EWC with SSL, we pre-compute and publicly release the Fisher Information Matrix (FIM), evaluated with 10,000 ImageNet-1K variates evaluated on large modern SSL architectures including ViT-B/16 and ResNe",10.1007/978-981-95-5696-0_24,"Lecture Notes in Computer Science, Pattern Recognition and Computer Vision",Springer Nature Singapore
http://arxiv.org/abs/1906.11951v1,Supervise Thyself: Examining Self-Supervised Representations in Interactive Environments,Evan Racah; Christopher Pal,2019-06-27,https://arxiv.org/pdf/1906.11951v1,"Self-supervised methods, wherein an agent learns representations solely by observing the results of its actions, become crucial in environments which do not provide a dense reward signal or have labels. In most cases, such methods are used for pretraining or auxiliary tasks for ""downstream"" tasks, such as control, exploration, or imitation learning. However, it is not clear which method's representations best capture meaningful features of the environment, and which are best suited for which types of environments. We present a small-scale study of self-supervised methods on two visual environments: Flappy Bird and Sonic The Hedgehog. In particular, we quantitatively evaluate the representations learned from these tasks in two contexts: a) the extent to which the representations capture true state information of the agent and b) how generalizable these representations are to novel situations, like new levels and textures. Lastly, we evaluate these self-supervised features by visualizing",10.1109/tnnls.2024.3386809,IEEE Transactions on Neural Networks and Learning Systems,Institute of Electrical and Electronics Engineers (IEEE)
http://arxiv.org/abs/2101.09825v1,Improving Few-Shot Learning with Auxiliary Self-Supervised Pretext Tasks,Nathaniel Simard; Guillaume Lagrange,2021-01-24,https://arxiv.org/pdf/2101.09825v1,"Recent work on few-shot learning \cite{tian2020rethinking} showed that quality of learned representations plays an important role in few-shot classification performance. On the other hand, the goal of self-supervised learning is to recover useful semantic information of the data without the use of class labels. In this work, we exploit the complementarity of both paradigms via a multi-task framework where we leverage recent self-supervised methods as auxiliary tasks. We found that combining multiple tasks is often beneficial, and that solving them simultaneously can be done efficiently. Our results suggest that self-supervised auxiliary tasks are effective data-dependent regularizers for representation learning. Our code is available at: \url{https://github.com/nathanielsimard/improving-fs-ssl}.",10.1609/aaai.v37i8.26148,Proceedings of the AAAI Conference on Artificial Intelligence,Association for the Advancement of Artificial Intelligence (AAAI)
